# MNN 本地模型运行

Operit AI 支持通过 MNN（Mobile Neural Network）框架在本地设备上运行 AI 模型，无需依赖云端 API，实现完全离线的 AI 对话体验。

> ⚠️ **重要提示**：本地运行模型的理解能力较差，性能远不如云端 API。建议在使用本地模型时关闭记忆链接、开启禁用工具进行对话，以获得更好的体验。

---

## 如何配置 MNN 本地模型

### 1. 选择 MNN 提供商

在"模型与参数配置"页面中，创建新的模型配置或编辑现有配置：

1. 进入 **"设置" -> "模型与参数配置"**
2. 点击 **"+ 新建"** 创建新配置，或选择现有配置进行编辑
3. 在 **"API提供商"** 中选择 **"MNN (本地推理)"**

![MNN 模型配置界面](/manuals/assets/model/fff4d73e9796dec22f22f0702babf62d.jpg)

### 2. 下载 MNN 模型

选择 MNN 提供商后，您会看到 **"下载MNN模型"** 按钮。点击该按钮进入模型下载页面。

![MNN 模型下载界面](/manuals/assets/model/82262fe261a5d48199ee575c08c91f21.jpg)

在模型下载页面中：

*   您可以通过搜索框搜索特定的模型
*   每个模型卡片显示：
    *   **模型名称**：如 `WebSailor-3B-MNN`、`Lingshu-7B-MNN`、`MiniCPM-V-4-MNN` 等
    *   **模型大小**：显示模型文件占用的存储空间（如 4.10 GB、7.00 GB、20.00 GB）
    *   **模型标签**：
        *   **Vision**：支持视觉理解的多模态模型
        *   **Think**：支持思考链推理的模型
*   点击 **"开始下载"** 按钮下载选定的模型

### 3. 配置模型参数

下载模型后，返回模型配置页面，您需要配置以下参数：

#### 选择模型配置

*   **模型名称**：从下拉列表中选择已下载的 MNN 模型，例如 `DeepSeek-R1-1.5B-Qwen-MN`
*   您可以点击 **"+ 新建"** 创建新的模型配置
*   使用 **"删除"** 按钮删除不需要的配置
*   使用 **"测试连接"** 验证模型是否正常工作

#### 计算类型

*   **CPU**：使用 CPU 进行推理（默认选项）
    *   兼容性最好，但速度较慢
    *   适合低端设备或没有 GPU 的设备

#### 线程数

*   设置用于模型推理的 CPU 线程数
*   默认值为 **8**
*   建议根据设备 CPU 核心数进行调整：
    *   4 核设备：建议设置为 4-6
    *   8 核设备：建议设置为 6-8
    *   更多核心：可以适当增加，但过多线程可能导致性能下降

---

## 使用建议

### 性能优化建议

1.  **关闭记忆链接**：本地模型理解能力有限，关闭记忆功能可以避免上下文混乱
2.  **禁用工具使用**：本地模型在处理复杂工具调用时表现不佳，建议在对话设置中禁用工具
3.  **选择合适的模型**：
    *   小模型（如 1.5B-3B）：速度快，但能力有限
    *   大模型（如 7B-20B）：能力更强，但需要更多存储空间和计算资源
4.  **调整线程数**：根据设备性能调整线程数，找到速度和稳定性的平衡点

### 适用场景

本地 MNN 模型适合以下场景：

*   **隐私保护**：需要完全离线运行，不将数据上传到云端
*   **网络受限**：在没有网络连接或网络不稳定的环境中使用
*   **成本控制**：避免 API 调用费用
*   **简单对话**：进行基础的文本对话，不需要复杂推理

### 不适用场景

以下场景建议使用云端 API：

*   **复杂任务**：需要深度理解、代码生成、复杂推理等任务
*   **工具使用**：需要 AI 调用各种工具完成复杂操作
*   **高质量对话**：需要高质量、流畅的对话体验
*   **多模态任务**：需要处理图像、音频等多模态内容（虽然部分 MNN 模型支持 Vision，但性能有限）

---

## 注意事项

*   **存储空间**：MNN 模型文件较大（通常几 GB 到几十 GB），请确保设备有足够的存储空间
*   **设备性能**：本地推理对设备性能要求较高，低端设备可能运行缓慢或无法运行
*   **电池消耗**：本地模型推理会消耗大量电量，建议在充电时使用
*   **发热问题**：长时间运行可能导致设备发热，请注意散热

---

通过合理配置和使用 MNN 本地模型，您可以在保护隐私和控制成本的同时，享受基本的 AI 对话功能。但对于需要高质量体验的场景，我们仍然建议使用云端 API 服务。

